{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef30d444",
   "metadata": {},
   "source": [
    "### **Term Paper Code: Supervised Classification**\n",
    "#### **Group - 1**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c29b1e",
   "metadata": {},
   "source": [
    "##### *Start by importing all neccessary Libraries*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "20542161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all necessary Libraries\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog, messagebox\n",
    "from tkinter import ttk\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import contextlib # Added for dummy torch context manager\n",
    "\n",
    "# --- Scikit-learn Imports ---\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, cohen_kappa_score\n",
    "from sklearn.neighbors import KNeighborsClassifier, NearestCentroid\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a4ea55ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing RPNet Specific Imports with exceptions handling\n",
    "# --- RPNet Specific Imports (Optional Dependency) ---\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.nn.functional as F\n",
    "    TORCH_AVAILABLE = True\n",
    "except ImportError:\n",
    "    TORCH_AVAILABLE = False\n",
    "    # Define dummy classes/functions if torch not found\n",
    "    class nn: Module = object; Parameter = object\n",
    "    class F:\n",
    "        @staticmethod\n",
    "        def conv2d(*args, **kwargs): raise ImportError(\"PyTorch not found. RPNet requires PyTorch installation.\")\n",
    "    class torch:\n",
    "        float32 = None\n",
    "        no_grad = contextlib.contextmanager(lambda: (yield))\n",
    "        @staticmethod\n",
    "        def tensor(*args, **kwargs): raise ImportError(\"PyTorch not found. RPNet requires PyTorch installation.\")\n",
    "        @staticmethod\n",
    "        def cat(*args, **kwargs): raise ImportError(\"PyTorch not found. RPNet requires PyTorch installation.\")\n",
    "        @staticmethod\n",
    "        def from_numpy(*args, **kwargs): raise ImportError(\"PyTorch not found. RPNet requires PyTorch installation.\")\n",
    "        @staticmethod\n",
    "        def unsqueeze(*args, **kwargs): raise ImportError(\"PyTorch not found. RPNet requires PyTorch installation.\")\n",
    "        @staticmethod\n",
    "        def squeeze(*args, **kwargs): raise ImportError(\"PyTorch not found. RPNet requires PyTorch installation.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2605151a",
   "metadata": {},
   "source": [
    "##### *Main*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0af3402f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating Global Variables for GUI\n",
    "# --- Global Variables ---\n",
    "corrected_img = None\n",
    "ground_truth = None\n",
    "\n",
    "\n",
    "# Creating RPNet Functions for Feature Extraction\n",
    "# --- RPNet Functions ---\n",
    "\n",
    "# Creating apply_pca_and_whiten for PCA, takes \n",
    "def apply_pca_and_whiten(X, p):\n",
    "    '''\n",
    "    Input Parameters:\n",
    "        X: feature map (R, C, N)\n",
    "        p: number of PCs to keep\n",
    "    Output Parameters:\n",
    "        X_white: whitened feature map (R, C, p) or (R, C, 0)\n",
    "        pca: PCA object fitted on the data or None\n",
    "    '''\n",
    "    if X.ndim != 3:\n",
    "        raise ValueError(f\"Input X must be 3D (R, C, N), but got shape {X.shape}\")\n",
    "    R, C, N = X.shape\n",
    "\n",
    "    if p <= 0: # Handle case where 0 components are requested explicitly or implicitly\n",
    "        print(f\"Warning: PCA requested with p={p}. Returning zero features.\")\n",
    "        return np.zeros((R, C, 0)), None # Return empty features and None for pca object\n",
    "\n",
    "    if N < p:\n",
    "        print(f\"Warning: Requested {p} PCA components, but input only has {N} features. Using n_components={N}.\")\n",
    "        p = N # Adjust p to the maximum possible\n",
    "\n",
    "    if N == 0: # Handle case where input has no features\n",
    "        print(f\"Warning: Input to PCA has 0 features. Returning zero features.\")\n",
    "        return np.zeros((R, C, 0)), None\n",
    "\n",
    "    reshaped = X.reshape(-1, N)\n",
    "\n",
    "    try:\n",
    "        pca = PCA(n_components=p)\n",
    "        X_pca = pca.fit_transform(reshaped)\n",
    "\n",
    "        # Check if PCA actually produced components (it might not if variance is zero)\n",
    "        if X_pca.shape[1] == 0:\n",
    "             print(f\"Warning: PCA resulted in 0 components for p={p} (input shape {N}). Returning zero features.\")\n",
    "             return np.zeros((R, C, 0)), pca # Return empty features\n",
    "\n",
    "        # Whiten (Standardize PCA components)\n",
    "        scaler = StandardScaler()\n",
    "        X_white_flat = scaler.fit_transform(X_pca)\n",
    "        # Ensure output shape matches requested 'p' (or adjusted 'p')\n",
    "        return X_white_flat.reshape(R, C, X_pca.shape[1]), pca\n",
    "    except ValueError as e:\n",
    "         # Catch errors during fit (e.g., all zero variance input)\n",
    "         print(f\"Error during PCA/Whitening (p={p}, N={N}): {e}. Returning zero features.\")\n",
    "         return np.zeros((R, C, 0)), None # Return empty on error\n",
    "\n",
    "\n",
    "# extract_random_patches_as_filters (minor robustness)\n",
    "def extract_random_patches_as_filters(X_white, patch_size, k):\n",
    "    ''' Extracts k random patches from X_white to be used as Conv filters. '''\n",
    "    if not TORCH_AVAILABLE:\n",
    "         raise ImportError(\"PyTorch not found. RPNet requires PyTorch installation.\")\n",
    "    if X_white.ndim != 3:\n",
    "        raise ValueError(f\"Input X_white must be 3D (R, C, P), but got shape {X_white.shape}\")\n",
    "\n",
    "    R, C, P = X_white.shape\n",
    "    if P == 0:\n",
    "        raise ValueError(\"Input to extract_random_patches_as_filters has 0 features (P=0). Cannot extract filters.\")\n",
    "\n",
    "    pad = patch_size // 2\n",
    "    # Use 'reflect' padding\n",
    "    padded = np.pad(X_white, ((pad, pad), (pad, pad), (0, 0)), mode='reflect')\n",
    "    patches = []\n",
    "    attempts = 0\n",
    "    max_attempts = k * 5 # Try a bit harder to find patches\n",
    "\n",
    "    while len(patches) < k and attempts < max_attempts:\n",
    "        attempts += 1\n",
    "        # Ensure indices are within valid range for the *original* dimensions R, C\n",
    "        # We sample center points from the original image grid\n",
    "        i_center = np.random.randint(0, R)\n",
    "        j_center = np.random.randint(0, C)\n",
    "        # Calculate slice indices in the *padded* array\n",
    "        i_start, i_end = i_center, i_center + patch_size\n",
    "        j_start, j_end = j_center, j_center + patch_size\n",
    "        patch = padded[i_start:i_end, j_start:j_end, :]\n",
    "\n",
    "        if patch.shape == (patch_size, patch_size, P):\n",
    "            # Transpose to (P, H, W) - Channels first for PyTorch Conv2d filters\n",
    "            patches.append(np.transpose(patch, (2, 0, 1)))\n",
    "        else:\n",
    "             # This case should be rare with correct padding and indexing\n",
    "             print(f\"Warning: Extracted patch shape mismatch. Expected {(patch_size, patch_size, P)}, got {patch.shape}. Indices i_center={i_center}, j_center={j_center}\")\n",
    "\n",
    "    if len(patches) < k:\n",
    "         print(f\"Warning: Could only extract {len(patches)} out of {k} desired patches.\")\n",
    "    if not patches:\n",
    "        raise ValueError(f\"Could not extract any valid patches after {max_attempts} attempts. Check patch_size ({patch_size}), input dimensions ({R},{C},{P}), and padding.\")\n",
    "\n",
    "    # Stack patches to form the filter bank: (num_extracted_patches, P, H, W)\n",
    "    filter_bank = np.stack(patches)\n",
    "    return torch.tensor(filter_bank, dtype=torch.float32)\n",
    "\n",
    "\n",
    "class RPNetFixedLayer(nn.Module):\n",
    "    def __init__(self, filters):\n",
    "        super(RPNetFixedLayer, self).__init__()\n",
    "        if not TORCH_AVAILABLE:\n",
    "             raise ImportError(\"PyTorch not found. RPNet requires PyTorch installation.\")\n",
    "        self.filters = nn.Parameter(filters, requires_grad=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if not isinstance(x, torch.Tensor):\n",
    "            x = torch.tensor(x, dtype=torch.float32)\n",
    "        elif x.dtype != torch.float32:\n",
    "            x = x.float()\n",
    "        # filters shape: (C_out=k, C_in=P, kH, kW)\n",
    "        return F.conv2d(x, self.filters, padding='same')\n",
    "\n",
    "# run_rpnet_layers\n",
    "def run_rpnet_layers(X_input, patch_size, k, L, initial_pca_components=4):\n",
    "    \"\"\" Runs RPNet. Returns (R, C, Features) or (R, C, 0) if fails. \"\"\"\n",
    "    if not TORCH_AVAILABLE:\n",
    "        raise ImportError(\"PyTorch not found. RPNet requires PyTorch installation.\")\n",
    "\n",
    "    print(f\"Running RPNet: patch_size={patch_size}, k={k}, L={L}, initial_pca={initial_pca_components}\")\n",
    "    R_orig, C_orig, N_orig = X_input.shape\n",
    "\n",
    "    # follow these steps to for running the RPNet:\n",
    "    # 1. Initial PCA and Whitening\n",
    "    print(\"  - Applying initial PCA...\")\n",
    "    current_feature_map, _ = apply_pca_and_whiten(X_input, initial_pca_components)\n",
    "    if current_feature_map.shape[-1] == 0:\n",
    "         print(\"  - Error: Initial PCA resulted in 0 features. Aborting RPNet.\")\n",
    "         return np.zeros((R_orig, C_orig, 0))                       # Return empty features\n",
    "    print(f\"  - Initial PCA output shape: {current_feature_map.shape}\")\n",
    "\n",
    "    feature_stack = []\n",
    "\n",
    "    for layer_idx in range(L):\n",
    "        print(f\"  - Processing RPNet Layer {layer_idx + 1}/{L}...\")\n",
    "        current_R, current_C, current_P = current_feature_map.shape\n",
    "\n",
    "        # 2. Intermediate PCA (if not first layer)\n",
    "        if layer_idx > 0:\n",
    "             print(f\"    - Applying intermediate PCA (target 4 components)...\")\n",
    "             intermediate_pca_target = 4\n",
    "             current_feature_map, _ = apply_pca_and_whiten(current_feature_map, intermediate_pca_target)\n",
    "             # Check if intermediate PCA failed\n",
    "             if current_feature_map.shape[-1] == 0:\n",
    "                 print(f\"    - Error: Intermediate PCA resulted in 0 features at layer {layer_idx + 1}. Stopping RPNet processing.\")\n",
    "                 break # Stop adding layers if features vanish\n",
    "             print(f\"    - Intermediate PCA output shape: {current_feature_map.shape}\")\n",
    "             # Update P for filter extraction\n",
    "             current_P = current_feature_map.shape[-1]\n",
    "\n",
    "        # 3. Extract Filters\n",
    "        print(f\"    - Extracting {k} random patches (size {patch_size}x{patch_size})...\")\n",
    "        try:\n",
    "            # Check if there are enough features to extract patches from\n",
    "            if current_P == 0:\n",
    "                print(\"    - Error: Cannot extract patches with 0 input features. Stopping layer processing.\")\n",
    "                break\n",
    "            filters = extract_random_patches_as_filters(current_feature_map, patch_size, k)\n",
    "            # Update k if fewer filters were extracted\n",
    "            actual_k = filters.shape[0]\n",
    "            print(f\"    - Actual filters extracted: {actual_k}. Shape: {filters.shape}\")\n",
    "        except ValueError as e:\n",
    "            print(f\"    - Error extracting patches: {e}. Stopping RPNet processing.\")\n",
    "            break # Stop if filters cannot be created\n",
    "\n",
    "        # 4. Prepare Input Tensor for Convolution\n",
    "        #    Input needs shape (1, P, R, C)\n",
    "        inp_tensor = torch.tensor(current_feature_map.transpose(2, 0, 1)).unsqueeze(0)\n",
    "        print(f\"    - Input tensor shape for Conv2D: {inp_tensor.shape}\")\n",
    "\n",
    "        # 5. Define and Apply Fixed Convolution Layer\n",
    "        model = RPNetFixedLayer(filters)\n",
    "        with torch.no_grad():\n",
    "            output_tensor = model(inp_tensor)               # Output shape (1, actual_k, R, C)\n",
    "\n",
    "        # 6. Process Output\n",
    "        output_map = output_tensor.squeeze(0).numpy().transpose(1, 2, 0) # Shape (R, C, actual_k)\n",
    "        print(f\"    - Output map shape for layer {layer_idx + 1}: {output_map.shape}\")\n",
    "\n",
    "        feature_stack.append(output_map)\n",
    "        current_feature_map = output_map                    # Output of this layer is input for next\n",
    "\n",
    "    # 7. Concatenate features\n",
    "    if not feature_stack:\n",
    "         print(\"Warning: RPNet generated no feature maps in any layer.\")\n",
    "         return np.zeros((R_orig, C_orig, 0)) # Return shape (R, C, 0)\n",
    "\n",
    "    try:\n",
    "        final_features = np.concatenate(feature_stack, axis=-1)\n",
    "        print(f\"  - Final RPNet features concatenated. Shape: {final_features.shape}\")\n",
    "        # Ensure final shape matches original spatial dimensions\n",
    "        if final_features.shape[:2] != (R_orig, C_orig):\n",
    "             print(f\"Warning: Final RPNet feature spatial dimensions {final_features.shape[:2]} don't match original {R_orig, C_orig}. This shouldn't happen with 'same' padding.\")\n",
    "             # Attempt to resize? Or return empty? For now, return empty.\n",
    "             return np.zeros((R_orig, C_orig, 0))\n",
    "        return final_features\n",
    "    except ValueError as e:\n",
    "         print(f\"Error concatenating RPNet features: {e}. Feature stack shapes: {[f.shape for f in feature_stack]}\")\n",
    "         return np.zeros((R_orig, C_orig, 0))\n",
    "\n",
    "# compile all the featurrses together in  using combined_features function \n",
    "def combined_features(spatial_feat, spectral_data):\n",
    "    \"\"\" Combines RPNet spatial features and original spectral data. Handles empty spatial_feat. \"\"\"\n",
    "    print(\"Combining features...\")\n",
    "    R, C, N = spectral_data.shape\n",
    "\n",
    "    # --- Handle Spectral Data ---\n",
    "    spectral_flat = spectral_data.reshape(-1, N)\n",
    "    scaler_spectral = StandardScaler()\n",
    "    spectral_scaled_flat = scaler_spectral.fit_transform(spectral_flat)\n",
    "    print(f\"  - Scaled spectral features shape: {spectral_scaled_flat.shape}\")\n",
    "\n",
    "    # --- Handle Spatial Data ---\n",
    "    if spatial_feat is None or spatial_feat.size == 0 or spatial_feat.shape[-1] == 0:\n",
    "        print(\"  - No valid spatial features provided. Using only spectral features.\")\n",
    "        # No spatial features, return only scaled spectral (already flat)\n",
    "        # We still need to scale them together IF spatial existed, but here only spectral exists.\n",
    "        # So, scale spectral alone.\n",
    "        scaler_combined = StandardScaler()\n",
    "        combined_scaled_flat = scaler_combined.fit_transform(spectral_scaled_flat)\n",
    "        print(f\"  - Final combined scaled flat shape (spectral only): {combined_scaled_flat.shape}\")\n",
    "        return combined_scaled_flat\n",
    "    else:\n",
    "        # Spatial features exist\n",
    "        S = spatial_feat.shape[-1]                                  # Number of spatial features\n",
    "        print(f\"  - Processing {S} spatial features.\")\n",
    "        spatial_flat = spatial_feat.reshape(-1, S)\n",
    "        scaler_spatial = StandardScaler()\n",
    "        spatial_scaled_flat = scaler_spatial.fit_transform(spatial_flat)\n",
    "        print(f\"  - Scaled spatial features shape: {spatial_scaled_flat.shape}\")\n",
    "\n",
    "        # --- Combine Scaled Features ---\n",
    "        combined_flat = np.concatenate([spectral_scaled_flat, spatial_scaled_flat], axis=-1)\n",
    "        print(f\"  - Combined flat shape before final scaling: {combined_flat.shape}\")\n",
    "\n",
    "        # Scale the combined features together\n",
    "        scaler_combined = StandardScaler()\n",
    "        combined_scaled_flat = scaler_combined.fit_transform(combined_flat)\n",
    "        print(f\"  - Final combined scaled flat shape: {combined_scaled_flat.shape}\")\n",
    "        return combined_scaled_flat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb3223c",
   "metadata": {},
   "source": [
    "#### *Create the Final GUI functions and GUI*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "459c83e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- GUI Functions ---\n",
    "# function to laod the corrected or Raw file\n",
    "def load_corrected_file():\n",
    "    global corrected_img\n",
    "    path = filedialog.askopenfilename(title=\"Select Corrected Image (.mat)\", filetypes=[(\"MAT files\", \"*.mat\")])\n",
    "    if path:\n",
    "        try:\n",
    "            data = scipy.io.loadmat(path)\n",
    "            potential_keys = [k for k, v in data.items() if isinstance(v, np.ndarray) and v.ndim == 3]\n",
    "            if not potential_keys:\n",
    "                raise ValueError(\"No 3D numpy array found in corrected MAT file.\")\n",
    "            img_key = max(potential_keys, key=lambda k: data[k].size)\n",
    "            corrected_img = data[img_key].astype(np.float32)            # Ensure float32\n",
    "            status_label.config(text=f\"✅ Corrected image loaded (key: '{img_key}')\", fg='green')\n",
    "            status_label2.config(text=f\"Shape: {corrected_img.shape}, Bands: {corrected_img.shape[2]}, Type: {corrected_img.dtype}\")\n",
    "            print(f\"Loaded corrected image: shape={corrected_img.shape}, dtype={corrected_img.dtype}\")\n",
    "        except Exception as e:\n",
    "            corrected_img = None\n",
    "            status_label.config(text=f\"❌ Error loading corrected image: {e}\", fg='red')\n",
    "            status_label2.config(text=\"\")\n",
    "            messagebox.showerror(\"Load Error\", f\"Failed to load corrected image:\\n{e}\")\n",
    "\n",
    "# function to load the Ground Truth File \n",
    "def load_gt_file():\n",
    "    global ground_truth\n",
    "    path = filedialog.askopenfilename(title=\"Select Ground Truth (.mat)\", filetypes=[(\"MAT files\", \"*.mat\")])\n",
    "    if path:\n",
    "        try:\n",
    "            data = scipy.io.loadmat(path)\n",
    "            potential_keys = [k for k, v in data.items() if isinstance(v, np.ndarray) and v.ndim == 2]\n",
    "            if not potential_keys:\n",
    "                raise ValueError(\"No 2D numpy array found in ground truth MAT file.\")\n",
    "            gt_key = max(potential_keys, key=lambda k: data[k].size)\n",
    "            ground_truth = data[gt_key]\n",
    "            ground_truth = ground_truth.astype(int)                     # Ensure GT is integer type\n",
    "            status_label.config(text=f\"✅ Ground truth loaded (key: '{gt_key}')\", fg='green')\n",
    "            status_label2.config(text=f\"Shape: {ground_truth.shape}, Classes: {len(np.unique(ground_truth))}\")\n",
    "        except Exception as e:\n",
    "            ground_truth = None\n",
    "            status_label.config(text=f\"❌ Error loading ground truth: {e}\", fg='red')\n",
    "            status_label2.config(text=\"\")\n",
    "            messagebox.showerror(\"Load Error\", f\"Failed to load ground truth:\\n{e}\")\n",
    "\n",
    "# Function for enabling PCA\n",
    "def toggle_pca_options(*args):\n",
    "    \"\"\"Enable/disable PCA components entry based on feature extraction choice.\"\"\"\n",
    "    if feature_extraction_method.get() == \"PCA\":\n",
    "        pca_label.config(state=tk.NORMAL)\n",
    "        pca_entry.config(state=tk.NORMAL)\n",
    "    else:\n",
    "        pca_label.config(state=tk.DISABLED)\n",
    "        pca_entry.config(state=tk.DISABLED)\n",
    "\n",
    "# --- classify_and_display Function ---\n",
    "def classify_and_display():\n",
    "    # (Initial checks for loaded data and matching dimensions are unchanged) ---> they will be highlighted in the status bar of our GUI\n",
    "    if corrected_img is None or ground_truth is None:\n",
    "        messagebox.showerror(\"Error\", \"Please load both corrected image and ground truth files.\")\n",
    "        status_label.config(text=\"❌ Load both corrected and ground truth files first!\", fg='red')\n",
    "        return\n",
    "    if corrected_img.shape[:2] != ground_truth.shape:\n",
    "         messagebox.showerror(\"Error\", f\"Image dimensions {corrected_img.shape[:2]} do not match Ground Truth dimensions {ground_truth.shape}.\")\n",
    "         status_label.config(text=\"❌ Image and Ground Truth dimensions do not match!\", fg='red')\n",
    "         status_label2.config(text=f\"Image: {corrected_img.shape[:2]}, GT: {ground_truth.shape}\")\n",
    "         return\n",
    "\n",
    "    feature_method = feature_extraction_method.get()\n",
    "    clf_name = classifier_type.get()\n",
    "\n",
    "    status_label.config(text=f\"⏳ Preparing data (Feature Method: {feature_method})...\", fg='blue')\n",
    "    status_label2.config(text=\"\")\n",
    "    gui_frame.update_idletasks()\n",
    "\n",
    "    try:\n",
    "        h, w, b = corrected_img.shape               # height, width , bands\n",
    "        gt_flat = ground_truth.ravel()              # sinle band image \n",
    "\n",
    "        # --- Feature Extraction ---\n",
    "        X_extracted_flat = None                     # Will hold the features for all pixels (h*w, num_features)\n",
    "        num_features = 0\n",
    "\n",
    "        # Apply method for feature extraction \n",
    "        # None ---> takes raw image and classifies\n",
    "        if feature_method == \"None\":\n",
    "            status_label2.config(text=\"Using raw spectral features.\")\n",
    "            gui_frame.update_idletasks()\n",
    "            X_extracted_flat = corrected_img.reshape(-1, b)\n",
    "\n",
    "        # PCA -----> for dimessionality reduction (faster)\n",
    "        elif feature_method == \"PCA\":\n",
    "            n_comp = pca_components.get()\n",
    "            if n_comp <= 0 or n_comp > b:\n",
    "                 messagebox.showerror(\"Error\", f\"Number of PCA components must be between 1 and {b}.\")\n",
    "                 status_label.config(text=f\"❌ Invalid PCA components (1-{b})\", fg='red')\n",
    "                 return\n",
    "            status_label.config(text=f\"⏳ Applying PCA ({n_comp} components)...\", fg='blue')\n",
    "            gui_frame.update_idletasks()\n",
    "            # Use apply_pca_and_whiten to get scaled PCA features directly\n",
    "            # We need the flat version, so reshape input and apply\n",
    "            pca_features_scaled_flat, pca_obj = apply_pca_and_whiten(corrected_img, n_comp)\n",
    "            # Reshape the output of apply_pca_and_whiten (which is R,C,P) back to flat\n",
    "            if pca_features_scaled_flat.shape[-1] == 0:\n",
    "                messagebox.showerror(\"Error\", f\"PCA resulted in 0 components.\")\n",
    "                status_label.config(text=f\"❌ PCA failed to produce components.\", fg='red')\n",
    "                return\n",
    "            X_extracted_flat = pca_features_scaled_flat.reshape(-1, pca_features_scaled_flat.shape[-1])\n",
    "            explained_variance = np.sum(pca_obj.explained_variance_ratio_) * 100 if pca_obj else 0\n",
    "            status_label2.config(text=f\"PCA Applied ({X_extracted_flat.shape[1]} comps). Var: {explained_variance:.2f}%\")\n",
    "            print(f\"PCA completed. Output shape: {X_extracted_flat.shape}\")\n",
    "\n",
    "        # RPNet ------> More Robust for Feature extraction in classification problems (does that by using patch selection)\n",
    "        elif feature_method == \"RPNet\":\n",
    "            if not TORCH_AVAILABLE:\n",
    "                messagebox.showerror(\"Dependency Error\", \"RPNet requires PyTorch.\\nPlease install it (e.g., `pip install torch`).\")\n",
    "                status_label.config(text=\"❌ RPNet requires PyTorch. Please install it.\", fg='red')\n",
    "                return\n",
    "\n",
    "            status_label.config(text=\"⏳ Running RPNet Feature Extraction (can take time)...\", fg='blue')\n",
    "            gui_frame.update_idletasks()\n",
    "\n",
    "            rp_patch_size = 20; rp_k = 10; rp_L = 3; rp_initial_pca = 4\n",
    "\n",
    "            spatial_features_map = run_rpnet_layers(corrected_img, rp_patch_size, rp_k, rp_L, rp_initial_pca) # Shape (R, C, k*L or 0)\n",
    "\n",
    "            # Check if RPNet produced any features before combining\n",
    "            if spatial_features_map.shape[-1] == 0:\n",
    "                 status_label2.config(text=\"RPNet failed to generate spatial features. Using spectral only.\")\n",
    "                 print(\"RPNet returned 0 features. Proceeding with spectral only for combination.\")\n",
    "                 # combined_features will handle the zero-feature case now\n",
    "            else:\n",
    "                status_label2.config(text=\"RPNet spatial features generated. Combining with spectral...\")\n",
    "\n",
    "            gui_frame.update_idletasks()\n",
    "            X_extracted_flat = combined_features(spatial_features_map, corrected_img)           # Shape (R*C, CombinedFeatures)\n",
    "\n",
    "            status_label2.config(text=f\"Feature combination complete. Total features: {X_extracted_flat.shape[1]}\")\n",
    "            print(f\"RPNet+Combined completed. Output shape: {X_extracted_flat.shape}\")\n",
    "\n",
    "\n",
    "        else:\n",
    "             messagebox.showerror(\"Error\", \"Unknown feature extraction method selected.\")\n",
    "             status_label.config(text=\"❌ Unknown feature extraction method selected.\", fg='red')\n",
    "             return\n",
    "\n",
    "        # --- Check if features were actually extracted ---\n",
    "        if X_extracted_flat is None or X_extracted_flat.size == 0:\n",
    "             messagebox.showerror(\"Error\", f\"Feature extraction ({feature_method}) failed to produce any features.\")\n",
    "             status_label.config(text=f\"❌ Feature extraction ({feature_method}) produced no data.\", fg='red')\n",
    "             return\n",
    "        num_features = X_extracted_flat.shape[1]\n",
    "        print(f\"Features extracted successfully: shape={X_extracted_flat.shape}\")\n",
    "\n",
    "\n",
    "        # --- Prepare Training/Prediction Data ---\n",
    "        status_label.config(text=\"⏳ Preparing training data...\", fg='blue')\n",
    "        gui_frame.update_idletasks()\n",
    "\n",
    "        if remove_bg.get(): train_mask = gt_flat > 0\n",
    "        else: train_mask = gt_flat >= 0\n",
    "\n",
    "        X_train_raw = X_extracted_flat[train_mask]\n",
    "        y_train = gt_flat[train_mask]\n",
    "\n",
    "        if X_train_raw.shape[0] == 0:\n",
    "             messagebox.showerror(\"Error\", \"No training samples found. Check ground truth or 'Exclude background' option.\")\n",
    "             status_label.config(text=\"❌ No training samples found with the current background setting.\", fg='red')\n",
    "             return\n",
    "\n",
    "        status_label2.config(text=f\"Training samples: {X_train_raw.shape[0]}, Features: {num_features}\")\n",
    "        gui_frame.update_idletasks()\n",
    "\n",
    "        # --- Scaling ---\n",
    "        # NOTE: The extracted features from PCA/RPNet might already be scaled.\n",
    "        # Re-scaling here ensures consistency, fitting on train and applying to all.\n",
    "        status_label.config(text=\"⏳ Scaling features (fit on train)...\", fg='blue')\n",
    "        gui_frame.update_idletasks()\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train_raw)\n",
    "        X_predict_scaled = scaler.transform(X_extracted_flat) # Use same scaler for prediction data\n",
    "\n",
    "        # now apply for classification \n",
    "        # --- Classification ---\n",
    "        # (Classification logic remains the same, using X_train_scaled and X_predict_scaled)\n",
    "        status_label.config(text=f\"⏳ Training {clf_name}...\", fg='blue')\n",
    "        gui_frame.update_idletasks()\n",
    "\n",
    "        clf = None\n",
    "        preds_flat = None                                     # Predictions for the entire image (flat)\n",
    "\n",
    "        unique_classes_train = np.unique(y_train)\n",
    "\n",
    "        # Classifier fitting and prediction logic \n",
    "        # for Minimum distance to means Classifier -----> Distance metric is Euclidean Distance \n",
    "        if clf_name == 'Minimum Distance':\n",
    "            clf = NearestCentroid(metric='euclidean')\n",
    "            clf.fit(X_train_scaled, y_train)\n",
    "            preds_flat = clf.predict(X_predict_scaled)\n",
    "\n",
    "        # For Parallelepiped classifier (created mmanually)\n",
    "        elif clf_name == 'Parallelepiped':\n",
    "             class_bounds = {}\n",
    "             preds_flat = np.zeros(X_predict_scaled.shape[0], dtype=int)\n",
    "             for c in unique_classes_train:\n",
    "                 if c == 0 and remove_bg.get(): continue\n",
    "                 X_c = X_train_scaled[y_train == c]\n",
    "                 if X_c.shape[0] > 0:\n",
    "                     min_vals = np.min(X_c, axis=0); max_vals = np.max(X_c, axis=0)\n",
    "                     class_bounds[c] = {'min': min_vals, 'max': max_vals}\n",
    "             for c in unique_classes_train:\n",
    "                  if c in class_bounds:\n",
    "                     is_within = np.all((X_predict_scaled >= class_bounds[c]['min']) & (X_predict_scaled <= class_bounds[c]['max']), axis=1)\n",
    "                     preds_flat[is_within & (preds_flat == 0)] = c\n",
    "        \n",
    "        # other classifiers created directly form skleaarn\n",
    "        # GML(NB) ---> Gaussina Maximum Likelihood Classifier using Naive Bayes (Assumes all classes to follow Gaussian Distribution)\n",
    "        elif clf_name == 'GML (NB)':\n",
    "            clf = GaussianNB()\n",
    "            clf.fit(X_train_scaled, y_train)\n",
    "            preds_flat = clf.predict(X_predict_scaled)\n",
    "        \n",
    "        # GML(QDA) ---> Gaussina Maximum Likelihood Classifier using Quadratic Discrimiantory Analysis (DOES NOT Assumes all classes to follow Gaussian Distribution) [Robust]\n",
    "        elif clf_name == 'GML (QDA)':\n",
    "             try:\n",
    "                 clf = QuadraticDiscriminantAnalysis()\n",
    "                 clf.fit(X_train_scaled, y_train)\n",
    "                 preds_flat = clf.predict(X_predict_scaled)\n",
    "             except Exception as qda_error:\n",
    "                  messagebox.showerror(\"QDA Error\", f\"QDA failed. This can happen if a class has too few samples for the number of features.\\nError: {qda_error}\")\n",
    "                  status_label.config(text=f\"❌ QDA Error: {qda_error}\", fg='red')\n",
    "                  return\n",
    "    \n",
    "        # K Nearest Neighbour Classifier (direct from Sklearn) \n",
    "        elif clf_name == 'KNN':\n",
    "            k = k_value.get()\n",
    "            if k <= 0:\n",
    "                messagebox.showerror(\"Error\", \"K value for KNN must be positive.\")\n",
    "                status_label.config(text=\"❌ K value must be positive.\", fg='red')\n",
    "                return\n",
    "            clf = KNeighborsClassifier(n_neighbors=k)\n",
    "            clf.fit(X_train_scaled, y_train)\n",
    "            preds_flat = clf.predict(X_predict_scaled)\n",
    "\n",
    "        # Suppoort Vector Machines Classifier (direct from Sklearn)\n",
    "        elif clf_name == 'SVM (RBF)':\n",
    "            clf = SVC(kernel='rbf', C=10, gamma='scale', probability=False)\n",
    "            clf.fit(X_train_scaled, y_train)\n",
    "            preds_flat = clf.predict(X_predict_scaled)\n",
    "\n",
    "        # check for predictions\n",
    "        if preds_flat is None:\n",
    "             messagebox.showerror(\"Error\", \"Classification step failed to produce predictions.\")\n",
    "             status_label.config(text=\"❌ Classification failed.\", fg='red')\n",
    "             return\n",
    "\n",
    "        classified_map = preds_flat.reshape(h, w)\n",
    "\n",
    "        # --- Evaluation (on training pixels) ---\n",
    "        status_label.config(text=\"⏳ Evaluating...\", fg='blue')\n",
    "        gui_frame.update_idletasks()\n",
    "        y_pred_train = preds_flat[train_mask]\n",
    "        labels_present = np.unique(np.concatenate((y_train, y_pred_train)))\n",
    "        cm = confusion_matrix(y_train, y_pred_train, labels=labels_present)\n",
    "        oa = accuracy_score(y_train, y_pred_train) * 100\n",
    "        kappa = cohen_kappa_score(y_train, y_pred_train)\n",
    "        status_label.config(text=\"✅ Classification Complete!\", fg='green')\n",
    "        status_label2.config(text=f\"OA: {oa:.2f}%, Kappa: {kappa:.4f} (evaluated on training pixels)\")\n",
    "\n",
    "\n",
    "        # --- Plotting --- (plot the result with accuracy and type of classifier)\n",
    "        fig, ax = plt.subplots(figsize=(7, 7))\n",
    "        max_class_val = np.max(ground_truth) if ground_truth is not None else 1\n",
    "        cmap = plt.cm.get_cmap('tab20', max_class_val + 1)\n",
    "        im = ax.imshow(classified_map, cmap=cmap, vmin=0, vmax=max_class_val)\n",
    "        ax.set_title(f\"{clf_name} ({feature_method}) | OA: {oa:.2f}% | Kappa: {kappa:.4f}\", fontsize=10)\n",
    "        ax.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # --- Exception Handling  ---\n",
    "    except ImportError as imp_err:\n",
    "         messagebox.showerror(\"Import Error\", f\"{imp_err}\\nMake sure required libraries (like PyTorch for RPNet) are installed.\")\n",
    "         status_label.config(text=f\"❌ Import Error: {imp_err}\", fg='red')\n",
    "    except Exception as e:\n",
    "        messagebox.showerror(\"Processing Error\", f\"An error occurred during {status_label.cget('text')}:\\n{e}\")\n",
    "        status_label.config(text=f\"❌ Error during processing: {e}\", fg='red')\n",
    "        status_label2.config(text=\"Check console for detailed traceback.\")\n",
    "        import traceback\n",
    "        print(\"\\n--- Error Traceback ---\")\n",
    "        traceback.print_exc()\n",
    "        print(\"---------------------\\n\")\n",
    "\n",
    "\n",
    "# --- GUI Setup  ---\n",
    "# Main (root) GUI\n",
    "gui_frame = tk.Tk()\n",
    "gui_frame.geometry(\"500x650\")\n",
    "gui_frame.resizable(False, False)\n",
    "gui_frame.title(\"Hyperspectral Classification GUI V2.1 (RPNet Fix)\")\n",
    "\n",
    "# --- Variables ---\n",
    "feature_extraction_method = tk.StringVar(value=\"None\")\n",
    "classifier_type = tk.StringVar(value=\"Minimum Distance\")\n",
    "k_value = tk.IntVar(value=5)\n",
    "pca_components = tk.IntVar(value=4)\n",
    "remove_bg = tk.BooleanVar(value=True)\n",
    "\n",
    "# --- Frames ---\n",
    "load_frame = ttk.LabelFrame(gui_frame, text=\"1. Load Data\")\n",
    "load_frame.pack(pady=10, padx=10, fill='x')\n",
    "feature_frame = ttk.LabelFrame(gui_frame, text=\"2. Feature Extraction Method\")\n",
    "feature_frame.pack(pady=5, padx=10, fill='x')\n",
    "classify_frame = ttk.LabelFrame(gui_frame, text=\"3. Classification Method\")\n",
    "classify_frame.pack(pady=5, padx=10, fill='x')\n",
    "options_frame = ttk.LabelFrame(gui_frame, text=\"4. Additional Options\")\n",
    "options_frame.pack(pady=5, padx=10, fill='x')\n",
    "run_frame = tk.Frame(gui_frame)\n",
    "run_frame.pack(pady=10, padx=10, fill='x')\n",
    "status_frame = ttk.LabelFrame(gui_frame, text=\"Status\")\n",
    "status_frame.pack(pady=5, padx=10, fill='x', expand=True)\n",
    "\n",
    "# --- Widgets ---\n",
    "# Load Frame\n",
    "tk.Button(load_frame, text=\"Load Corrected Image (.mat)\", command=load_corrected_file).pack(pady=5, padx=10, fill='x')\n",
    "tk.Button(load_frame, text=\"Load Ground Truth (.mat)\", command=load_gt_file).pack(pady=5, padx=10, fill='x')\n",
    "\n",
    "# Feature Frame\n",
    "tk.Label(feature_frame, text=\"Select Method:\").grid(row=0, column=0, padx=5, pady=5, sticky='w')\n",
    "feature_dropdown = ttk.Combobox(feature_frame, textvariable=feature_extraction_method, values=[\"None\", \"PCA\", \"RPNet\"], state=\"readonly\", width=15)\n",
    "feature_dropdown.grid(row=0, column=1, padx=5, pady=5, sticky='w')\n",
    "feature_dropdown.bind(\"<<ComboboxSelected>>\", toggle_pca_options)\n",
    "pca_label = tk.Label(feature_frame, text=\"PCA Components:\")\n",
    "pca_label.grid(row=1, column=0, padx=5, pady=2, sticky='e')\n",
    "pca_entry = tk.Entry(feature_frame, textvariable=pca_components, width=5)\n",
    "pca_entry.grid(row=1, column=1, padx=5, pady=2, sticky='w')\n",
    "toggle_pca_options() # Initial state\n",
    "\n",
    "# Classification Frame\n",
    "tk.Label(classify_frame, text=\"Select Classifier:\").grid(row=0, column=0, padx=5, pady=5, sticky='w')\n",
    "classifier_dropdown = ttk.Combobox(classify_frame, textvariable=classifier_type, values=['Minimum Distance', 'Parallelepiped', 'GML (NB)', 'GML (QDA)', 'KNN', 'SVM (RBF)'], state=\"readonly\", width=25)\n",
    "classifier_dropdown.grid(row=0, column=1, columnspan=2, padx=5, pady=5, sticky='w')\n",
    "knn_label = tk.Label(classify_frame, text=\"K value (for KNN only):\")\n",
    "knn_label.grid(row=1, column=0, padx=5, pady=5, sticky='e')\n",
    "knn_entry = tk.Entry(classify_frame, textvariable=k_value, width=5)\n",
    "knn_entry.grid(row=1, column=1, padx=5, pady=5, sticky='w')\n",
    "\n",
    "# Options Frame\n",
    "bg_check = tk.Checkbutton(options_frame, text=\"Exclude background (GT=0) for Training\", variable=remove_bg)\n",
    "bg_check.pack(padx=5, pady=5, anchor='w')\n",
    "\n",
    "# Run Frame\n",
    "tk.Button(run_frame, text=\"Run Classification and Display\", command=classify_and_display, font=(\"Arial\", 10, \"bold\")).pack(pady=10)\n",
    "\n",
    "# Status Frame\n",
    "status_label = tk.Label(status_frame, text=\"Load files to begin.\", fg='blue', wraplength=450, justify=\"left\", anchor='nw')\n",
    "status_label.pack(pady=2, padx=5, fill='x')\n",
    "status_label2 = tk.Label(status_frame, text=\"\", fg='darkblue', wraplength=450, justify=\"left\", anchor='nw')\n",
    "status_label2.pack(pady=2, padx=5, fill='x')\n",
    "\n",
    "# --- Main Loop ---\n",
    "gui_frame.mainloop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
